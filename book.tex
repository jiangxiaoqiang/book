\documentclass{book}

%%%%%% Import Package %%%%%%
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fontspec,xunicode,xltxtra}
\usepackage{xeCJK}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage[xindy]{glossaries}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{longtable}

%When compile under liunx% 
%\setmainfont{WenQuanYi Micro Hei}  
%\setmainfont{WenQuanYi Zen Hei Mono} 

\newcounter{coderemarks}   %创建变量
\setcounter{coderemarks}{1}   %设置变量初值为1
\newcounter{codevar}     %创建变量
\setcounter{codevar}{1}     %设置变量初值为

\newcommand{\circlemark}[1]{%
\tikz\node[text=white,font=\sffamily\bfseries,inner sep=0.2mm,draw,circle,fill=black]{#1};}

\newcommand{\makeremark}[1]{%
\circlemark{\arabic{coderemarks}}%
\global \expandafter\def \csname codebox\the\value{coderemarks}\endcsname{#1}%
\stepcounter{coderemarks}}

\newcommand{\showremarks}{%
\begin{list}{\circlemark{\arabic{codevar}}} %
{} %
\whiledo{\value{codevar} < \value{coderemarks}}{ %
\item \expandafter\csname codebox\the\value{codevar}\endcsname %
\stepcounter{codevar}} %
\end{list} %
\setcounter{coderemarks}{1}%
\setcounter{codevar}{1}%
}

\definecolor{orange}{RGB}{255,127,0} 
\definecolor{SpringGreen4}{RGB}{0,139,69}

\renewcommand{\figurename}{图}
\renewcommand{\tablename}{表}
\renewcommand{\contentsname}{目录}

\interfootnotelinepenalty=10000

\newcommand*{\songti}{\CJKfamily{zhsong}} % 宋体
%%%% Set section Attribute %%%%
\makeatletter
\makeatother

%%%% 设置 subsection 属性 %%%%
\makeatletter
\makeatother

%%%% 设置 subsubsection 属性 %%%%
\makeatletter
\makeatother

%Set Hyperref Format
\hypersetup{pdfborder={0 0 0}, colorlinks=true,linkcolor=blue}

% 段落首行缩进两个字 %
\makeatletter
\let\@afterindentfalse\@afterindenttrue
\@afterindenttrue
\makeatother

\setlength{\parindent}{2em}  %中文缩进两个汉字位

%%%% 下面的命令重定义页面边距，使其符合中文刊物习惯 %%%%
\addtolength{\topmargin}{-54pt}
\setlength{\oddsidemargin}{0.63cm}  % 3.17cm - 1 inch
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{14.66cm}
\setlength{\textheight}{24.00cm}    % 24.62

%%%%设置TOC深度%%%%
\setcounter{tocdepth}{4}

%%%% 下面的命令设置行间距与段落间距 %%%%
\linespread{1.4}
% \setlength{\parskip}{1ex}
\setlength{\parskip}{0.5\baselineskip}

%Set where to find the graphics%
\graphicspath{{./Image/common/}{./Image/Api/}{./Image/InterfaceDesign/}{./Image/Attachment/}}
%\graphicspath{{I:\Nutstore\Document\Tex\JsptpdCSSDS\Image\Interface\_Design}}%

%Set Code Format%
\lstloadlanguages{C, csh, make,python}
\lstset{	  
	 alsolanguage= XML,  
	 tabsize=4, %  
	 frame=shadowbox, %把代码用带有阴影的框圈起来  
	 commentstyle=\color{red!50!green!50!blue!50},%浅灰色的注释  
	 frameround=tttt,%圆角边框
	 rulesepcolor=\color{red!20!green!20!blue!20},%代码块边框为淡青色  
	 keywordstyle=\color{blue!90}\bfseries, %代码关键字的颜色为蓝色，粗体  
	 showstringspaces=false,%不显示代码字符串中间的空格标记  
	 stringstyle=\ttfamily, % 代码字符串的特殊格式  
	 keepspaces=true, %  
	 breakindent=22pt, %  
	 numbers=left,%左侧显示行号 往左靠,还可以为right，或none，即不加行号  
	 stepnumber=1,%若设置为2，则显示行号为1,3,5，即stepnumber为公差,默认stepnumber=1  
	 %numberstyle=\tiny, %行号字体用小号  
	 numberstyle={\color[RGB]{0,192,192}\tiny} ,%设置行号的大小，大小有tiny,scriptsize,footnotesize,small,normalsize,large等  
	 numbersep=8pt,  %设置行号与代码的距离，默认是5pt  
	 basicstyle=\footnotesize, % 这句设置代码的大小  
	 showspaces=false, %  
	 flexiblecolumns=true, %  
	 breaklines=true, %对过长的代码自动换行  
	 breakautoindent=true,%  
	 breakindent=4em, %  	   
	 aboveskip=1em, %代码块边框  
	 tabsize=4,  
	 showstringspaces=false, %不显示字符串中的空格  
	 backgroundcolor=\color[RGB]{245,245,244},   %代码背景色  
	 %backgroundcolor=\color[rgb]{0.91,0.91,0.91}    %添加背景色  
	 %escapeinside=``,  %在``里显示中文  
	 %% added by http://bbs.ctex.org/viewthread.php?tid=53451  
	 fontadjust,  
	 captionpos=t,  
	 framextopmargin=2pt,
	 framexbottommargin=2pt,
	 abovecaptionskip=-3pt,
	 belowcaptionskip=3pt,  
	 xleftmargin=4em,
	 xrightmargin=4em, % 设定listing左右的空白  
	 texcl=true	 
}

%\preamble

%%%% Generate glossary %%%%
\makeglossaries

\newglossaryentry{computer}
{
	name=computer,
    description={aa}
}

%%%% 正文开始 %%%%
\begin{document}
%封面%
\begin{titlepage}
\begin{center}

\textsc{\LARGE Experience Record}\\[1.5cm]

\textsc{Dolphin Project}\\[0.5cm]

\rule{8cm}{0.2em}\mbox{} \\[0.4cm]

{\huge \bfseries Record}\\[0.4cm]

\rule{8cm}{0.2em}\mbox{} \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Xiaoqiang \textsc{Jiang}
\end{flushleft}
\end{minipage}

\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Xiaoqiang \textsc{Jiang}
\end{flushright}
\end{minipage}

\vfill

{\large \today}

\end{center}
\end{titlepage}

%\begin{CJK}{UTF8}{gbsn}
%%%% 定义标题格式，包括title，author，affiliation，email等 %%%%
\title{\Huge{Cruise}		
\author{蒋小强\footnote{本文档由\LaTeX{}生成，作者
\texttt{mail:jiangtingqiang@gmail.com}
}}}

\date{2015.04}	

%%%% Generate Title %%%%  
\maketitle %
\clearpage
\mbox{}         
\clearpage

\begin{table}\caption[Caption for LOF]{修改记录\protect\footnotemark}					
	\medskip
	\centering		
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multirow{1}{*}{序号}
		& \multicolumn{1}{c|}{修改人}  
		& \multicolumn{1}{c|}{修改日期} 
		& \multicolumn{1}{c|}{备注}\\			
		\cline{1-4}
		1 & 蒋小强 & 2015-04-22 & 创建基础版本\\
		\hline	
	\end{tabular}
\end{table}

\footnotetext{表格\LaTeX{}代码生成可到此处\url{http://www.tablesgenerator.com/}}

\clearpage

\clearpage
\mbox{}         
\clearpage
	
\tableofcontents	

\part{第一部}


\chapter{第一章}

\section{常见问题排查及解决}

\subsection{U盘只读(Read-only file system)}

在拷贝文件到U盘时，提示如下：

\begin{lstlisting}[language=Bash]
cp: cannot create regular file '/run/media/dolphin/Fedora-WS-Live-28-1-1/note': Read-only file system
\end{lstlisting}

回想了下，应该是不久前U盘做了Fedora的刻录盘，刻录工将U盘设置为读保护模式，此时可以使用如下命令将U盘设置为读写模式：

\begin{lstlisting}[language=Bash]
# 设置设备/dev/sdb1为读写模式
sudo blockdev --setrw /dev/sdb1
# 设置设备/dev/sdb1为只读(read only哦)模式
sudo blockdev --setro /dev/sdb1
\end{lstlisting}

blockdev命令可以获取Linux下块设备的属性值，以及设置一些块设备的属性值。/dev/sdb1是文件系统名字。设置U盘为读写模式后可顺利拷贝文件到U盘。比较诡异的是，在运行了block命令后，是可以拷贝文件。但是后来不和何原因，U盘又变成只读了，而且blockdev命令只读设置不再有效。后面的解决方式就是将U盘重新分区\footnote{参考StackOverflow的解方法:\url{https://unix.stackexchange.com/questions/216152/usb-disk-read-only-cannot-format-turn-off-write-protection}}，做以下操作之前提前备份文件。列出当前系统上的所有分区信息：

\begin{lstlisting}[language=Bash]
fdisk -l
\end{lstlisting}

在列出的分区表中，找到U盘的设备名字，这里是/dev/sdb1。输入如下命令准备编辑分区信息：

\begin{lstlisting}[language=Bash]
fdisk /dev/sdb1
\end{lstlisting}

根据提示，删除旧分区信息，创建新分区信息即可。输入d(delete)删除旧分区，输入n(new)创建新分区，输入w(write)保存新分区。使用如下命令创建FAT文件系统：

\begin{lstlisting}[language=Bash]
mkfs.vfat -F 32 /dev/sdb1
\end{lstlisting}

\subsection{RestTemplate read time out}

运行一段时间后，系统翻页，偶尔返回空数据。经检查，服务端在使用RestTemplate调用接口时，偶尔会返回Read time out错误。Spring RestTemplate的Read time out默认超时时间是1秒，如图\ref{fig:resttempldatedefaulttimeout}所示。当接口的响应时间超过1秒时，会出现read time out错误。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{resttempldatedefaulttimeout.png}
	\caption{RestTemplate默认超时时间}
	\label{fig:resttempldatedefaulttimeout}
\end{figure}

上图是Debug跟踪时查看的RestTemplate默认变量。由图中可以看出，默认超时时间是1秒。此时需要增大Read time out的默认超时时间。自定义Read time out超时时间如下代码片段所示：

\begin{lstlisting}[language=Java]
@Bean(name = "commonRestTemplate")
public RestTemplate restTemplate(RestTemplateBuilder restTemplateBuilder) {
	return restTemplateBuilder.setConnectTimeout(5000)
			.setReadTimeout(20000)
			.build();
}
\end{lstlisting}

出现此问题的原因是：当数据量较少时，请求基本能在非常短时间内响应，当数据量逐渐增加时，部分请求处理时间偶尔超过1秒，会出现偶尔失败的情况。

\subsection{自动化部署(Auto Deploy)}

\subsubsection{整体概况}

目前已经将自动化部署应用到部分项目中，省去了非常多手工操作。目前应用的项目情况一览表，表中，CI表示Continuous Integration。

\begin{table}[htbp]
	\caption{自动部署项目信息}
	\label{table:databaseconnectionpool}
	\begin{center}
		\begin{tabular}{|c|c|p{5cm}|}
			\hline
			\multirow{1}{*}{项目名称}
			& \multicolumn{1}{c|}{中文名} 
			& \multicolumn{1}{c|}{说明}\\			
			\cline{1-3}
			report &  通用数据录入子系统  & 通过可灵活调整的配置，动态快速应对不同维度的数据录入场景 \\
			\hline
			report-frontend & 通用数据录入子系统(前端) & 通过可灵活调整的配置，动态快速应对不同维度的数据录入场景，提供动态渲染的，统一的UI \\
			\hline
			system & 主应用 & 主应用 \\
			\hline
			system-frontend & 主应用(前端) & 主应用(前端) \\
			\hline
			system-exchange & 数据交换(exchange)子系统 & 应对所有数据交换场景 \\
			\hline
			system-api & 接口(api)应用 & 接口应用，提供外部数据服务 \\
			\hline
			message & 消息系统 & 站内通知 \\
			\hline
			message-frontend & 消息系统(前端) & 站内通知 \\
			\hline
			web-ci & Web(Continuous Integration) & 网站 \\
			\hline
			web-ci-59 & Web & 网站 \\
			\hline
			web-ci-for-bid & Web & 网站(定制版) \\
			\hline				
		\end{tabular}	
	\end{center}
\end{table}

自动化部署服务器信息如表\ref{table:databaseconnectionpool}所示。提交代码后，构建服务器定时检查代码更新，检查周期可以通过cron表达式定义，若有变更，则触发自动编译构建。应用构建并打包完毕后，会通过调用文件中转服务，将编译完成的应用包拷贝到测试服务器指定目录下。测试服务器上会有定时任务定时调用Shell脚本，Shell脚本会监测应用文件的变化，或者版本定义文件中版本号的变化，任意一项有变动，则触发应用更新。版本变化通过读取版本配置文件来侦测，文件变化通过对比文件的Hash来判断。

\begin{table}[htbp]
	\caption{自动部署服务器信息}
	\label{table:databaseconnectionpool}
	\begin{center}
		\begin{tabular}{|c|c|p{7cm}|}
			\hline
			\multirow{1}{*}{IP}
			& \multicolumn{1}{c|}{名称} 
			& \multicolumn{1}{c|}{备注}\\			
			\cline{1-3}
			192.168.1.24 &  Jenkins服务器  & 所有项目的构建、编译、打包 \\
			\hline
			192.168.1.11 & Nginx服务器 & 文件的接收转发服务 \\
			\hline
			192.168.1.6 & App测试服务器 & 所有App运行在此服务器 \\
			\hline				
		\end{tabular}	
	\end{center}
\end{table}


\subsubsection{构建服务(Auto Build)}

构建服务包含获取源码更新、构建、构建后操作3步。由于项目针对不同应用场景，定义了不同的分支，所以在构建时，需要了解每个分支的含义。各分支定义如表\ref{table:projectbranch}所示：

\begin{table}[htbp]
	\caption{分支名称及含义}
	\label{table:projectbranch}
	\begin{center}
		\begin{tabular}{|c|c|p{5cm}|}
			\hline
			\multirow{1}{*}{项目名称}		 
			& \multicolumn{1}{c|}{分支名称}
			& \multicolumn{1}{c|}{备注}\\			
			\cline{1-3}
			system &  v1.3 & 主分支,1表示第几期，3表示阶段 \\
			\hline
			system & v1.3\_api & 接口分支 \\		
			\hline
			system & v1.3\_exchange & 数据交换分支 \\		
			\hline
			report & develop & 主分支 \\		
			\hline
			report-frontend & develop & 主分支 \\		
			\hline							
		\end{tabular}	
	\end{center}
\end{table}

构建服务定时检查源码仓库的变动，监测到变动后，触发自动编译打包等操作。构建后，通过调用构建后脚本，将文件拷贝到服务器。构建后脚本路径/home/jenkins-bak/，各个脚本的含义如表\ref{table:handlernamingrule}日所示。

\begin{table}[htbp]
	\caption{自动部署后触发脚本命名规则}
	\label{table:handlernamingrule}
	\begin{center}
		\begin{tabular}{|c|p{5cm}|}
			\hline
			\multirow{1}{*}{脚本名称}		 
			& \multicolumn{1}{c|}{备注}\\			
			\cline{1-2}
			\{prject-name\}-after-build-handler.sh &  项目后端构建后处理 \\
			\hline
			\{project-name\}-frontend-after-build-handler.sh  & 项目后端构建后处理 \\		
			\hline				
		\end{tabular}	
	\end{center}
\end{table}

脚本命名含义一般是项目名称加需要产生的作用，project-name表示对应的项目名字，新增时可替换为项目的实际名称。after-build-handler表示此脚本主要告诉Jenkins构建后需要执行此脚本完成一系列动作。构建需要特殊处理的地方：


\begin{lstlisting}[language=Bash]
#!/usr/bin/env bash

# 当使用未初始化的变量时，程序自动退出
set -u

# 当任何一行命令执行失败时，自动退出脚本
set -e

# 在运行结果之前，先输出执行的那一行命令
set -x

readonly APP_ID=""
readonly APP_KEY=""
readonly PROJECT_DIR="/var/lib/jenkins/workspace/web-ci-59"
readonly BUILD_OUTPUT_DIR="/var/lib/jenkins/workspace/web-ci-59/cms/target"

BUILD_DIST_FILENAME=${BUILD_OUTPUT_DIR}/cms.war

cd ${BUILD_OUTPUT_DIR}

#
# 文件过大,无法拷贝
# 将大文件拆分
#
split -b 80M ${BUILD_DIST_FILENAME}

CURRENT_TIME=`date '+%Y-%m-%d %H:%M:%S'`

CONST_STR="ysf"

#
# 生成TOKEN
# 接口各项认证参数的排列顺序是：
# 时间戳(timestamp)、AppKey、随机字符串(echostr)
#
TOKEN=`echo -n ${APP_ID}${APP_KEY}${CURRENT_TIME}${CONST_STR}|md5sum|awk '{print $1}'`
SEQUENCE_TOKEN=`echo -n ${CURRENT_TIME}${APP_ID}${APP_KEY}${CONST_STR}|shasum -a 1|awk '{print $1}'`


# 请求服务端,上传主文件
COMMAND=`curl -H "APPID:$APP_ID" \
-H "TIMESTAMP:$CURRENT_TIME" \
-H "ECHOSTR:$CONST_STR" \
-H "TOKEN:$SEQUENCE_TOKEN" \
-F "file=@${BUILD_OUTPUT_DIR}/xaa" \
http://192.168.1.11:8083/api/fileExchange/upload`

COMMAND=`curl -H "APPID:$APP_ID" \
-H "TIMESTAMP:$CURRENT_TIME" \
-H "ECHOSTR:$CONST_STR" \
-H "TOKEN:$SEQUENCE_TOKEN" \
-F "file=@${BUILD_OUTPUT_DIR}/xab" \
http://192.168.1.11:8083/api/fileExchange/upload`
\end{lstlisting}

在上传文件时，由于限制了文件大小(估计是100MB)，当部署文件大于100MB时，使用split命令，拆分成多个小文件分开上传，上传后通过cat命令组装成war包，组装后的文件与原始文件一致，可以通过对比文件的MD5值来判断。

\subsubsection{中转服务(Forward Service)}

中转服务将构建服务器上生成的应用包转发到应用服务器，应用包在测试服务器192.168.1.6的存放路径是/home/deploy/credit。中转服务的配置主要是Nginx转发上传的请求到指定服务器：

\begin{lstlisting}[language=Bash]
location /api/fileExchange {
	proxy_pass http:ip:port;
	proxy_redirect off;
}
\end{lstlisting}

\subsubsection{应用更新服务(Update Trigger)}

应用更新服务在测试服务器，通过建立cron定时任务，轮询检查应用更新情况。定时任务配置文件是/etc/crontab，各个系统更新触发脚本路径是/opt/app/script。定时任务配置示例：

\begin{lstlisting}[language=Bash]
#
# project name update trigger
#
*/1 * * * * root /opt/app/script/project-name-trigger.sh
\end{lstlisting}

触发更新规则在相应的Shell脚本中实现。如下代码片段所示：

\begin{lstlisting}[language=Bash]
#!/usr/bin/env bash

# 部署触发器
# 定时检查文件修改
# 文件修改后，触发部署动作

# 当使用未初始化的变量时，程序自动退出
#set -u

# 当任何一行命令执行失败时，自动退出脚本
set -e

# 在运行结果之前，先输出执行的那一行命令
set -x

# 定义错误日志级别
LOG_LEVEL=-9000

#定义日志存放目录
SIMPLE_LOG_4_SH_DIR=/tmp/simplelog4sh

#导入日志
. /opt/app/script/log4shell.sh

# 当前运行程序版本
readonly APP_PATH="/opt/app/backend-v1.3"
readonly DEPLOY_PATH="/home/deploy/credit"
source ${APP_PATH}/report-version.properties
CURRENT_VERSION=${VERSION}
source ${DEPLOY_PATH}/report-version.properties
DEPLOY_VERSION=${VERSION}
readonly FILE_NAME="report-web-boot-${CURRENT_VERSION}.jar"
readonly DEPLOY_FILE_NAME="report-web-boot-${DEPLOY_VERSION}.jar"

logInfo "检查后端App更新..."

deploy()
{
	logInfo "停止旧版本程序...,版本：${CURRENT_VERSION}"
	ps -ef|grep -w ${FILE_NAME}|grep -v grep|cut -c 9-15|xargs kill 9
	logDebug "开始拷贝新版程序文件....."
	yes|cp -rf ${DEPLOY_PATH}/${DEPLOY_FILE_NAME} ${APP_PATH}
	yes|cp -rf ${DEPLOY_PATH}/credit-report-version.properties ${APP_PATH}
	logInfo "启动新版程序......,程序路径:${APP_PATH},版本：${DEPLOY_VERSION}"
	${APP_PATH}/start-v1.3.sh
}

if test ${CURRENT_VERSION} = ${DEPLOY_VERSION}
then
	logInfo "版本号无变化，检查文件Hash...."
	CURRENT_VERSION_MD5=`md5sum ${APP_PATH}/${FILE_NAME}|cut -d ' ' -f1`
	DEPLOY_VERSION_MD5=`md5sum ${DEPLOY_PATH}/${FILE_NAME}|cut -d ' ' -f1`
	if test ${CURRENT_VERSION_MD5} != ${DEPLOY_VERSION_MD5}
	then
		deploy ""
	else
		logInfo "Hash无变化，文件未修改，后端App结束..."
	fi
else
	logInfo "后端App版本有变化,开始部署新版程序..."
	deploy ""
fi
\end{lstlisting}

由于目前应用较多，单独列出每个应用的目录比较冗长。此处仅仅描述目录的命名的一般性原则。测试环境的目录统一在opt下(项目初始阶段习惯)，正式环境的目录在根目录home(初始习惯)或者data下，data目录存放应用是推荐的标准做法，以后新应用部署推荐用此标准，data下存放组织名称对应的目录，组织名称下对应此组织相应的应用。当前各个应用的目录如表\ref{table:projectdirectionayinfo}所示。recommand表示推荐做法，obsolete表示过时的做法，以后不推荐采用的方式。Test表示测试环境，对应的测试服务器IP,Production表示生产环境，对应的生产环境的IP。

\begin{table}[htbp]
	\caption{项目部署目录信息}
	\label{table:projectdirectionayinfo}
	\begin{center}
		\begin{tabular}{|c|c|p{5cm}|c|}
			\hline
			\multirow{1}{*}{IP}
			& \multicolumn{1}{c|}{环境} 
			& \multicolumn{1}{c|}{目录}
			& \multicolumn{1}{c|}{备注}\\			
			\cline{1-4}
			192.168.1.6 &  Test  & /opt/app/ & 部署目录 \\
			\hline
			192.168.1.6 &  Test  & /home/deploy/credit/ & CI部署文件存放目录 \\
			\hline
			192.168.1.6 &  Test  & /opt/app/script/ & CI部署脚本存放目录 \\
			\hline
			10.10.1.* &  Production  & /home/app/ & \textcolor{red}{obsolete} \\
			\hline
			10.10.1.* &  Production  & /data/\{companyname\}/app/ & \textcolor{green}{recommand} \\
			\hline							
		\end{tabular}	
	\end{center}
\end{table}

\subsection{Nginx超时转发}

在后端有双机或多台机器提供服务的情况下，如果某一台机器超过了一定时间未响应，Nginx将尝试将请求转发到下一台服务器。由于写操作时，如果没有在后端作重复校验，一旦写操作比较耗时，转发后会出现重复写的情况(POST一般不进行超时转发，但是幂等请求下也可能会产生重复，比如日志记录、触发消息通知等等)。避免此问题的一种设计方案是在前端渲染表单时，生成一个唯一ID，防止重复提交。所以此处仅仅将超时转发的规则应用在部分url上，在Nginx中的配置如下：

\begin{lstlisting}[language=Bash]
upstream example_upstream{
	server 192.168.0.1 max_fails=1 fail_timeout=3s;
	server 192.168.0.2 max_fails=1 fail_timeout=3s backup;
}
location /example/ {
	proxy_pass http://example_upstream/;
	proxy_set_header Host: test.example.com;
	proxy_set_head X-real-ip $remote_addr;
	proxy_next_upstream error timeout http_500;
}
\end{lstlisting}

Nginx在POST, LOCK, PATCH这种会对服务器造成不幂等的方法，默认是不进行重试的，如果一定要进行重试，则要加上如下配置：

\begin{lstlisting}[language=Bash]
# 非幂等也进行超时转发配置
proxy_next_upstream error timeout http_500 non_idemponent;
\end{lstlisting}




\subsection{无法获取数据库连接}

\begin{quote}
	\textbf{\textcolor{red}{导致问题的原因：数据库连接配置过小，访问量增长时造成连接分配不足，导致无法顺畅访问站点。}}
\end{quote}

最近2天网站运行一段时间后会突然宕掉，日志输出无法获取数据库连接(Could not get JDBC connection)，初步推测是数据库连接泄漏，造成数据库可用连接被使用完，主备节点都拿不到连接，造成此问题，目前连接由连接池进行管理，一般情况下是不需要人工干预数据库的连接申请和释放，也有可能是某处进行了手工申请数据库连接，但是没有释放连接导致。使用如下Shell脚本每隔一段时间检测连接池数量：

\begin{lstlisting}[language=Bash]
# 每5秒查看一次到数据库连接个数
nohup watch -n 5 'lsof -i:5236|wc -l >> pool.log'
\end{lstlisting}

经过观察，数据库连接稳定在300-310之间，如果是连接池泄漏，那么连接会随着程序运行逐渐增长，最终达到连接数量上限，根据观察，初步可以排除连接泄漏导致此问题。后来经过日志筛查，发现如下输出：

\begin{lstlisting}[language=Bash]
# 活动连接数
active 20,maxActive 20
\end{lstlisting}

表示当前活动连接数20个，最大活动连接数20个。虽然到数据库的会话有300多个，但是绝大多数是空闲的(Idle)，如下语句查看数据库当前会话的个数。

\begin{lstlisting}[language=SQL]
-- 查看数据库当前会话情况
select clnt_ip,state,user_name,count(*)
from v$sessions
group by clnt_ip,state,user_name
\end{lstlisting}

结果如图\ref{fig:sessionstatistics}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{sessionstatistics.png}
	\caption{DB当前连接会话统计}
	\label{fig:sessionstatistics}
\end{figure}

可以看出，活动的会话稳定在13个，离最大个数20已经不远，极有可能是数据库会话配置过小，导致用户增多时，无可用连接分配。为了验证推断，使用JMeter\footnote{\url{http://jmeter.apache.org/}}模拟多用户同时访问测试网站。逐步增加同时访问的用户的个数，当增加到80个时，网站无响应。检查后端日志，出现了线上的问题同样的日志输出，说明问题在此处，需要优化。DBCP当连接数超过最大连接数时，所有连接都会被断开[未找到权威出处]。经过考虑，逐步应用以下优化的方式。

\paragraph{增加网站数据库连接数量上限}调整配置增大数据库连接上限，可以增加网站并发服务能力：

\begin{lstlisting}[language=XML]
<!-- 最小空闲连接数 -->
<property name="minIdle" value="50" />
<!-- 最大连接数 -->
<property name="maxActive" value="100" />
\end{lstlisting}

做了以上优化后，可以提高网站的并发服务量，降低由于访问量过大造成网站不可用的概率，20个连接确实太小，综合评估数据库现有的连接资源(500)，调整后连接数分配情况：

\begin{table}[htbp]
	\caption{应用连接个数分配情况}
	\label{table:projectdirectionayinfo}
	\begin{center}
		\begin{tabular}{|c|c|p{2cm}|}
			\hline
			\multirow{1}{*}{环境}
			& \multicolumn{1}{c|}{IP} 
			& \multicolumn{1}{c|}{连接个数}\\			
			\cline{1-3}
			主机 &  192.168.1.1  & 100 \\
			\hline
			备机 &  192.168.1.23  & 100 \\
			\hline
			移动端 &  192.168.1.1  & 100 \\
			\hline
			旧版App、定制版App、登陆、临时使用预留 &  192.168.1.*  & 200 \\
			\hline									
		\end{tabular}	
	\end{center}
\end{table}

增加了连接数上限后，为了验证效果，采用crontab定时任务每隔5分钟定时采集不同时间点数据库的活动Session个数：

\begin{lstlisting}[language=Bash]
# 每5分钟定时执行采集任务
*/5 *  *  *  *  root   /usr/local/dolphin/script/db-pool-monitor.sh
\end{lstlisting}

脚本将采集到的数据放入csv文件中，后期采用Excel生成可直观的图表：

\begin{lstlisting}[language=Bash]
#!/usr/bin/env bash

# 当使用未初始化的变量时，程序自动退出
set -u

# 当任何一行命令执行失败时，自动退出脚本
set -e

# 在运行结果之前，先输出执行的那一行命令
set -x

connect_size=`lsof -i|grep 10.20.1.17|grep java|wc -l`
current_time=`date "+%H:%M:%S"`
current_date=`date "+%Y%m%d"`
echo "${current_time},${connect_size}" >> /home/monitor/dm-pool-"${current_date}".csv
\end{lstlisting}

数据库实时Session监控脚本向csv中写入2列，当前的时间和当前时间点的活动Session的个数，每天生成一个csv文件。根据csv生成的折线图效果如图\ref{fig:dbsession}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{dbsession.bmp}
	\caption{DB历史会话数统计}
	\label{fig:dbsession}
\end{figure}

8:50到9:40时间段进行了压力测试，这个时间范围内数据库的活动Session有明显的增加。

\paragraph{增大数据库Session数量上限}目前数据库的会话数量是500，可以调整为1000。

\paragraph{缓存页面信息}实际上，页面在绝大部分时间是不会改变的，完全可以将页面信息缓存在Redis中，不但减轻数据库压力，还可成倍提高网站响应速度和并发处理能力。此种优化方式需要对代码做一定程度改造，比如发布信息时，需要清除对应模块的Redis缓存信息，保证信息更新及时。

\paragraph{将流量分配到不同的机器}后端流量可以主备机同时启用，涉及到Session共享问题，需要进一步测试。




\subsection{Java heap space}

\begin{quote}
	\textbf{\textcolor{red}{导致问题的原因：表面看是APP分配的内存资源不足，系统并发性能不足，应对持续并发请求能力需要加强，实际是Tomcat请求头配置过大(100MB)，每个线程会申请对应大小的空间，而大对象JVM直接会放到老年代，一旦并发请求稍多，造成内存溢出。}}
\end{quote}

上周网站Down掉，本周接口Down掉，愉快的周末泡汤了。所以，暗自忖度，一定要找到问所在。Heap的大小是Young Generation 和Tenured Generaion 之和。在JVM中如果98\%的时间是用于GC,且可用的Heap size 不足2\%的时候将抛出此异常信息。内存溢出一般分为2种情况，一时是超出预期的访问量/数据量，另一种是内存泄露(Memory leak)。第一次的内存溢出异常有hprof文件，将文件拷贝出来进行分析。第二次接口异常没有生成hprof，采用VisualVM\footnote{VisualVM在JDK安装包中附带，在\$JAVA\_HOME/bin目录下。}分析内存情况。新建

\begin{lstlisting}[language=Bash]
grant codebase "file:/home/local/jdk1.8.0_111/lib/tools.jar" {   
	permission java.security.AllPermission;   
};
\end{lstlisting}

在服务端启动jstatd(Java State Daemon)守护进程:

\begin{lstlisting}[language=Bash]
# 启动统计守护进程
./jstatd -J-Djava.security.policy=../jstatd.all.policy -J-Djava.rmi.server.hostname=10.10.1.53
# 生成内存dump文件
jmap -dump:format=b,file=/opt/example.dump [pid]
\end{lstlisting}

在本地使用VisualVM监视服务端JVM的运行情况，内存的增长速度在4-5MB/s,2GB的内存在4-5min就被消耗完毕，接着进行一次YGC(Young Garbage Collection)。初步监测，老年代(Old Generation)的内存的确在不断增长。但是仅凭此点，无法下结论一定存在内存泄漏，如果老年代在以后执行GC(Garbage Collection)后，老年代(Old Generation)初始内存不停增长，才可以推断的确存在内存泄漏，因为不停增加的初始内存，标志着有部分内存永远无法被FGC回收，随着程序的不断运行，必定会由于内存不足而Hang住。

\begin{table}[htbp]
	\caption{应用内存消耗}
	\label{table:appmemoryusing}
	\begin{center}
		\begin{tabular}{|c|c|p{2cm}|}
			\hline
			\multirow{1}{*}{并发数量}
			& \multicolumn{1}{c|}{Old Generation内存消耗} 
			& \multicolumn{1}{c|}{连接个数}\\			
			\cline{1-3}
			0 &  404MB  & 100 \\
			\hline
			1 &  1.119GB  & 100 \\
			\hline
			10 &  3.847GB  & 100 \\
			\hline
			20 &  3.689GB  & 100 \\
			\hline
			30 &  4.834GB  & 100 \\
			\hline											
		\end{tabular}	
	\end{center}
\end{table}

在一段时期内，程序一启动老年代就已经满了，占用率直接100\%。开始是以为流量较高，YGC频繁，不停的有经过多次YGC未回收的对象移入老年代，最多FGC执行频繁一些罢了。后来更为诡异的事情发生了，机器突然变得非常卡顿，磁盘使用量快速增长，最后程序直接无法启动，启动不到30s立即停止响应或者直接输出Java Heap space错误。

\paragraph{问题复现}

经过备份的日志阅读，发现一段时间段内，有大量针对某一查询请求，且非常集中。下午抽出时间在测试环境使用JMeter构造红集中请求的查询，模拟线上场景。当并发的用户量不断增长时，YGC越来越频繁，同时老年代初始内存消耗上升快速。当并发量达到50时，老年代内存使用量飙升到100\%,同时程序Hang住，输出Java Heap space错误。终于知道症结在机器在应对并发流量时内存不足，没有对内存做好估算。并发量升高时，应用对的内存消耗极大的超出了预期，即使每个线程5MB的内存，100个并发也才500MB，当前的内存有5GB，按照预期是完全足够的。但最终还是将服务器的内存增大到64GB来解决问题。

\paragraph{问题回顾}

当在并发量较高时，APP启动时，会占用比较多的老年代内存，所以在下午应用已启动老年代内存立即耗尽，年轻代不停的执行GC。当并发超出系统承载能力时，老年代默认内存已经无法支撑应用启动的初始内存消耗，所以应用启动立即发生内存溢出。有于在发生内存溢出时会生成dump文件，将现场保留到磁盘以备问题排查，每个dump文件从2GB-8GB不等，所以造成磁盘使用量急速消耗。

\begin{lstlisting}[language=Bash]
# 内存溢出时生成dump文件
-XX:+HeapDumpOnOutOfMemoryError 
-XX:HeapDumpPath=/home/app
\end{lstlisting}

不停的尝试启动，不停的生成dump文件，造成磁盘IO急剧升高，所以机器卡顿。

\paragraph{问题解决}

一是增大系统并发吞吐量，二是系统需要有限流和熔断机制。为了避免类似情况，系统做如下优化：

\begin{itemize}
	\item {一是将热点查询做成静态化页面，针对一级页面，打开即默认加载的数据静态化，避免后端接口处理，降低接口压力。}
	\item {二是缓存凭据，凭据信息更改不频繁，直接缓存到内存中，避免数据库查询。}
	\item {三是评估内存消耗，针对设计预期的并发分配相应的资源。}
	\item{四是限流，识别无效请求并主动丢弃，减少无效流量对资源的占用和消耗，可以根据实时流量调整限制级别。例如在平时网站运行时，可以将限制关闭，在应对重大事件时，可以将限制开启并根据网站保障级别调整限制级别。}
	\item {五是使用监控工具，对网站的实时流量和服务器负载有监控预警，根据流量情况采取不同的措施。}
\end{itemize}

关键的问题是，在当前并发的情况下，不应该消耗如此大的内存。问题在哪里？使用JMap\footnote{\url{https://docs.oracle.com/javase/7/docs/technotes/tools/share/jmap.html}}(Java Memory Map)命令将运行时内存情况输出，采用MAT\footnote{\url{https://www.eclipse.org/mat/}}(The Eclipse Memory Analyzer)工具进行分析，查看内存中的大对象如图\ref{fig:javahprofanalysis}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{javahprofanalysis.png}
	\caption{Java内存对象分析}
	\label{fig:javahprofanalysis}
\end{figure}

从Histogram图中可以看出，对象中byte数组的大小超过了8GB，每一个byte数组元素大小是102408208(约102MB)。怎么会有如此大对象？问题非常有可能就出在这些大对象上面了。下载Tomcat 8.0.53(注意下载的源码需要尽量服务器上运行的版本一致，InternalNioInputBuffer类在往后版本的Tomcat已经被移除)源码查看，找到如下代码片段：

\begin{lstlisting}[language=Java]
@Override
protected void init(SocketWrapper<NioChannel> socketWrapper,
AbstractEndpoint<NioChannel> endpoint) throws IOException {

	socket = socketWrapper.getSocket();
	if (socket == null) {
		// Socket has been closed in another thread
		throw new IOException(sm.getString("iib.socketClosed"));
	}
	socketReadBufferSize =
	socket.getBufHandler().getReadBuffer().capacity();
	
	int bufLength = headerBufferSize + socketReadBufferSize;
	if (buf == null || buf.length < bufLength) {
		buf = new byte[bufLength];
	}
	
	pool = ((NioEndpoint)endpoint).getSelectorPool();
}
\end{lstlisting}

在处理请求的线程初始化时，根据请求头大小新构建一个byte数组(\textcolor{red}{new byte[bufLength]})，原来是请求头设置过大。系统在生成byte数组时，判断是大对象，直接将对象放到老年代中，造成老年代消耗内存急速增长。

\subsection{报表在现场演示环境无法打开}

\begin{quote}
	\textbf{\textcolor{red}{导致问题的原因：报表地址配置的内网地址，公网用户无法访问。}}
\end{quote}

报表服务部署在内网的机器上，内网的机器与平时使用的机器在一个局域网中，内网用户访问报表可以直接通过内网IP。但是公网的用户无法访问报表服务器的内网IP，需要通过公网IP做一次转发。就像在自己家里部署一个服务，但是在办公室的PC是无法访问的，需要有公网IP做转发。公网用户访问报表的一般流程是，先访问公网地址，公网地址将流量转发到内网的反向代理服务器，反向代理服务器根据报表的URL将流量路由到报表服务器。使用如下语句更新报表配置即可：

\begin{lstlisting}[language=SQL]
# 将报表服务的IP调整为公网IP
update report_config
set url = replace(url,'内网IP','公网IP')
\end{lstlisting}

\subsection{查询变慢}

\begin{quote}
	\textbf{\textcolor{red}{导致问题的原因：日志数据不断膨胀造成查询变慢，日志记录的方式需要优化。}}
\end{quote}

这两天不断有反馈原来很快的查询现在不知道什么原因变慢了，查看了变慢的页面后，确实速度不符合设计预期，正常情况下不应大于200ms，平均响应时间肯定不会超过1s，页面数据是放在接口缓存中。加上调用链路的开销，不应该有延迟。访问接口数据的流程如图\ref{fig:api-request}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.1]{api-request.jpg}
	\caption{接口数据调用流程}
	\label{fig:api-request}
\end{figure}

首先确定接口缓存有效，在接口服务器上请求接口服务：

\begin{lstlisting}[language=Bash]
curl -H "APPID:" \ 
-H "TIMESTAMP:2016-12-19 16 (tel:2016121916):58:02" \ 
-H "ECHOSTR:sdsaasf" -H "TOKEN:" \
-H "Accept: application/json, text/plain" \ 
http://192.168.1.11:28081/api/example?name=\&page=1\&size=10|jq '.'
\end{lstlisting}

在50ms内可以返回结果，说明缓存有效，不是从数据库获取。其次确定网络设备影响(已经感受到的发送POST请求会有1-2s左右延迟，网络A的服务器发出请求，网络B的服务器需要1-2s才能实际接收到并处理)，在网络A使用访问接口服务，时间与网络B访问相近。由于端口80绑定域名，新建内网服务，从内网站点访问接口，时间超过3s，说明问题在A网的站点。后经过跟踪，发现A网站点接口本身处理非常快，但是绝大部分时间消耗在处理请求后返回的过程中。经过跟踪，处理请求后查询更新了日志表，而日志表的数据在800W左右。

\paragraph{问题解决}

\begin{itemize}
	\item {快速的解决方式，临时备份日志，建立空的日志表。}
	\item {重构日志，直接记录流水，不记录统计信息。(\textcolor{green}{\textbf{Recommand}})}
	\item {构建独立的日志服务，所有的日志打到独立的服务上，采用简单消息队列。(\textcolor{green}{\textbf{Recommand}})}
	\item {尝试ELK日志分析系统。}
\end{itemize}

\subsection{资源分配约定}

由于最近处理的问题大多涉及到资源问题，例如文件无法上传，邮件无法发送，主要由于磁盘空间不足。原来分配的虚拟机一般home目录较大，所以将随着程序运行可能需要消耗较大磁盘空间的目录放在home目录下，但是新内网虚拟机恰好home目录很小。导致一些列问题，所以这里约定新部署App目录的统一规范。

\begin{table}[htbp]
	\caption{磁盘目录分配约定}
	\label{table:}
	\begin{center}
		\begin{tabular}{|c|p{5cm}|}
			\hline
			\multirow{1}{*}{路径}
			& \multicolumn{1}{c|}{用途}\\			
			\cline{1-2}
			/opt/alibaba/app &  存放项目应用程序  \\
			\hline
			/opt/alibaba/var/data & 存放数据文件 \\
			\hline			
			/opt/alibaba/var/image & 存放图片文件 \\
			\hline
			/opt/alibaba/local & 存放三方程序 \\
			\hline
			/opt/alibaba/backup & 存放备份文件 \\
			\hline
			/opt/alibaba/logs & 存放日志文件 \\
			\hline														
		\end{tabular}	
	\end{center}
\end{table}

考虑到安全，程序启动须使用alibaba帐号，不使用root启动应用程序。日志应独立存放，避免放到应用目录下，因为日志文件通常会比较大，独立存放日志好处之一是在新应用部署时，可以直接拷贝app目录，而避免与日志一起拷贝或者需要单独做一步忽略日志的操作。




\subsection{Tomcat请求头设置}

App在启动的时候占用老年代内存较大，当并发增大时，消耗内存迅速增加。经过排查，是maxHttpHeaderSize参数配置过大。

\begin{lstlisting}[language=Bash]
#
# 请求头允许的最大长度，单位为KB，如果没有指定，默认是8192(8KB)
# 这里请适当斟酌，在批量查询时，URI中需要带参数(批量查询的参数会比较大)
# 点击返回按钮后，需要根据URI中记录的参数定位到上一个页面
# 如果不增加Header Size，没有更好的方案实现返回功能
# 所以Header Size设置得较大
#
server.max-http-header-size=102400000
\end{lstlisting}

这种问题应该是很明显的，为何到现在才发现？对比了旧版的配置，旧版配置的是1MB。估计在新版配置的时候顺手将开发环境的配置拷贝到生产，而开发环境的配置比较随意。Tomcat在处理HTTP请求时，会直接申请maxHttpHeaderSize大小的内存空间，而不仅仅是一个限制。


\subsection{GoAccess}

最近网站流量“偏高”，导致关键服务停服。所以可以考虑实时的监控网站的健康状态，作出对应的决策。由于安全原因不能使用监控应用，所以找到一个直接在终端下可以查看Nginx日志统计信息的应用GoAccess\footnote{\url{https://goaccess.io}}。

\begin{lstlisting}[language=Bash]
goaccess -a -d -f /usr/local/nginx/logs/example.log
goaccess -a -d -f /usr/local/nginx/logs/example.log -p /etc/goaccess.conf -o /data/html/hexo/public/go-access.html
\end{lstlisting}

不能以服务的方式运行，所以做一个crontab定时任务，每隔一段时间生成网站访问日志的统计信息：

\begin{lstlisting}[language=Bash]
# 每20分钟执行
*/20 * * * *   root   goaccess -d -f /usr/loca/nginx/logs/example.log -p /usr/local/etc/goaccess.conf
\end{lstlisting}

根据URL的访问频率统计信息如图\ref{fig:spideranalysis}所示，从统计图中可以看出，九成以上的请求都集中在一个URL。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{spideranalysis.png}
	\caption{访问URL统计}
	\label{fig:spideranalysis}
\end{figure}

根据IP的统计信息如图\ref{fig:ipstatistics}所示，从统计图中可以看出，5成以上的访问请求只服务了3个IP。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{ipstatistics.png}
	\caption{根据IP统计}
	\label{fig:ipstatistics}
\end{figure}

\subsection{Nginx启用压缩}

最近演示时说系统很慢，打开系统查看，首屏加载时间超过8s，确实不符合常规。由于网站是SPA应用，首先查看Javascript大小，竟然有接近8MB大小，对比了线上系统，发现演示系统Javascript没有压缩。这里经过了两级Ngnix代理，内网访问Ngnix代理，浏览器中查看Javascript大小在1.6MB左右，说明内网的Nginx代理已经压缩了Javascript文件，推测可能是另一级需要对代理Nginx做启用gzip压缩配置，尝试在另一级Nginx代理添加如下配置：

\begin{lstlisting}[language=Bash]
# HTTP代理版本(proxy\_http\_version)
proxy_http_version 1.1
# gzip支持版本(gzip\_http\_version)
gzip_http_version 1.0
\end{lstlisting}

gzip模块默认支持压缩的HTTP版本是1.1，而代理模块的默认代理HTTP版本是1.0\footnote{\url{https://reinout.vanrees.org/weblog/2015/11/19/nginx-proxy-gzip.html}}，不匹配，造成gzip的压缩没有生效，调整任意一项即可。

\subsection{查看网站并发}

使用如下命令查看网站并发：

\begin{lstlisting}[language=Bash]
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'

# 查看Ngni并发
tail -f access.log | awk -F '[' '{print $2}' | awk 'BEGIN{key="";count=0}{if(key==$1){count++}else{printf("%s\t%d\r\n", key, count);count=1;key=$1}}'
\end{lstlisting}

结果如图\ref{fig:websiteconcurrentaccess}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{websiteconcurrentaccess.png}
	\caption{查看服务器并发示例}
	\label{fig:websiteconcurrentaccess}
\end{figure}

\begin{lstlisting}[language=Bash]
goaccess -a -d -f /data/logs/fanhaobai.com.access.log -p /etc/goaccess.conf -o /data/html/hexo/public/go-access.html
\end{lstlisting}

不能以服务的方式运行，所以做一个crontab定时任务，每隔一段时间生成网站访问日志的统计信息：

\begin{lstlisting}[language=Bash]
# 每20分钟执行
*/20 * * * *   root   goaccess -d -f /usr/loca/nginx/logs/example.log -p /usr/local/etc/goaccess.conf
\end{lstlisting}

根据URL的访问频率统计信息如图\ref{fig:spideranalysis}所示，从统计图中可以看出，九成以上的请求都集中在一个URL。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{spideranalysis.png}
	\caption{访问URL统计}
	\label{fig:spideranalysis}
\end{figure}

根据IP的统计信息如图\ref{fig:ipstatistics}所示，从统计图中可以看出，5成以上的访问请求只服务了3个IP。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{ipstatistics.png}
	\caption{根据IP统计}
	\label{fig:ipstatistics}
\end{figure}

\subsection{自动构建更新的问题}

有时经常会遇到代码提交后在测试环境总看不到效果的问题。可以使用如下方式逐步排查。第一步检查Jenkins是否获取到了最新的代码，在Jenkins构建目录(/var/lib/jenkins/workspace)下检查最新代码片段。第二步检查Tomcat目录下是否包含最新的调整后的代码，由于Tomcat下是已经编译完毕的class文件，需要借助Luyten\footnote{Luyten是一个出色的Java反编译器,项目地址是\url{https://github.com/deathmarine/Luyten}。}工具打开，查看源码，是否包含最新的改动，如果包含最新改动，说明自动编译构建没问题。这里查看Tomcat下的class文件包含了最新的调整，但是没有自动更新，重启Tomcat后更新生效。尝试调整发布脚本，每次发布时重新启动Tomcat来解决此问题。

\begin{lstlisting}[language=Bash]
#!/usr/bin/env bash

# 当使用未初始化的变量时，程序自动退出
set -u

# 当任何一行命令执行失败时，自动退出脚本
set -e

# 在运行结果之前，先输出执行的那一行命令
set -x

readonly AUTO_BUILD_OUTPUT_PATH="/var/lib/jenkins/workspace/dolphin/api/target"
readonly PROGRAM_NAME="cms.war"
readonly DEPOLY_PATH="/opt/alibaba/local/apache-tomcat-8.0.52/webapps/"
readonly TOMCAT_BIN_PATH="/opt/alibaba/local/apache-tomcat-8.0.52/bin"
readonly APP_FILE_IDENTITY_NAME="apache-tomcat-8.0.52"

# 停止站点
ps -ef|grep -w ${APP_FILE_IDENTITY_NAME}|grep -v grep|cut -c 9-15|xargs kill 9

sleep 3

# 拷贝部署文件
yes|cp -rf ${AUTO_BUILD_OUTPUT_PATH}/cms.war ${DEPOLY_PATH}

# 启动站点
${TOMCAT_BIN_PATH}/startup.sh
\end{lstlisting}

\subsection{Non-resolvable parent POM for Could not find artifact and 'parent.relativePath' points at wrong local POM}

Maven构建时提示如下错误：

\begin{lstlisting}
Non-resolvable parent POM for Could not find artifact and 'parent.relativePath' points at wrong local POM
\end{lstlisting}

在Windows->Proference->Maven->User Settings中，调整Maven的配置文件的位置为D:/maven3/maven3/conf/settings.xml，重新构建即可。另外在项目中，某一个项目中没有Maven Dependency，导致依赖包无法找到，在项目的.classpath文件中添加如下配置即可：

\begin{lstlisting}[language=XML]
<classpathentry kind="con" path="org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER">
	<attributes>
		<attribute name="maven.pomderived" value="true"/>
		<attribute name="org.eclipse.jst.component.nondependency" value=""/>
	</attributes>
</classpathentry>
\end{lstlisting}

.classpath文件用于记录项目编译环境的所有信息，包括：源文件路径、编译后class文件存放路径、依赖的jar包路径、运行的容器信息、依赖的外部project等信息。另查看Maven Repositor在菜单Window->Show View->Other...->Maven->Maven Repositories下。



\chapter{部署}


\subsection{Platform}


关闭防火墙：

\begin{lstlisting}[language=Bash]
systemctl stop firewalld.service
\end{lstlisting}


\section{编辑中}

\subsection{LocalDateTime格式化}

在Spring中，接收LocalDateTime日期时间数据时，只需要使用@DateTimeFormat注解即可。由于没有加注解，返回的时间格式如下：


\begin{lstlisting}
{
	"message": "ok",
	"code": 2000,
	"data": {
	"dtexample": [
			2018,
			9,
			12,
			22,
			12,
			45,
			347000000
		]
	}
}

\end{lstlisting}

在Model的字段上添加时间格式的注解：

\begin{lstlisting}[language=Java]
@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyy-MM-dd HH:mm:ss", timezone = "GMT+8")
\end{lstlisting}

返回的时间格式如下：

\begin{lstlisting}
{
	"message": "ok",
	"code": 2000,
	"data": {
		"dtexample": "2018-09-12 22:13:16"
	}
}
\end{lstlisting}

\subsection{TCPCopy}

这里需要实现生产服务器抓取到流量，在测试服务器上进行回放，回放时可以适当放大流量，使程序实际具有更强的处理能力。TCPCopy可分为在线模式和离线模式，离线模式首先在线上服务器抓包，再拷贝到测试服务器进行重放。使用命令dump流量：

\begin{lstlisting}[language=Bash]
tcpdump -i eth0 -w /tmp/xxx.cap
\end{lstlisting}



\subsection{Could not get a resource from the pool}

网站运行一段时间后，无法访问，查看日志，出现如下提示：

\begin{lstlisting}[language=Bash]
redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
…
Caused by: java.util.NoSuchElementException: Pool exhausted
at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:464)
\end{lstlisting}

JedisPool中的Jedis对象个数是有限的，默认是8个。

https://yq.aliyun.com/articles/236384

\subsection{服务限流}

由于服务器的硬件资源和网络带宽资源始终是有限的，服务器可以承载的流量也会有上限，超过应用的承载极限后，程序稳定服务即无法保障。所以除了提高服务器服务能力外，还需要在超过服务器的承载上限之前，限制过量的访问请求，保护应用。网站许多内容缓存在Redis中，主要保障访问数据库连接不要超过数据库提供的最大连接上限(目前数据库提供的会话为100)，或者说不导致数据库过载。接口应用不要超过接口所能提供的最高并发。接口服务可以在Ngnix通过ngx\_http\_limit\_req\_module配置来实现，保障打到接口的流量不会超过接口可以承载的最大流量(经初步测试，在内存40GB左右的情况下，目前接口并发能到100左右，具体数量要根据接口的业务员逻辑复杂情况而定)。同时网站侧需要作流量统计识别与限制，防止无效流量占用接口资源导致接口疲于应付爬虫等无效流量而无法服务于正常用户。

\paragraph{网站流量识别与限制}

网站不采取在Ngnix上做流量限制，原因是



\paragraph{接口Nginx防止流量过载}


优化后，在测试环境作压力测试检验。



\subsection{Python绘图}


https://www.jianshu.com/p/d9cc124d8a30

\begin{lstlisting}[language=Bash]
pip install numpy
pip install scipy
dnf install python-matplotlib
\end{lstlisting}


\subsection{XX-Net}



redis为了避免客户端连接数过多，有一个timeout配置，意思是如果连接的空闲时间超过了timeout的值，则关闭连接。默认配置是0，意思是没有超时限制，永远不关闭连接。

\subsection{fluxion}

Handshake Snooper进行抓包嗅探。Monitor哦被动监听模式，只有对方主动连接目标时才会，抓到包，比较被动，不确定性很大，但是没有攻击性。第二种是常用的方式，使用Aircrack—ng强制解除认证，迫使目标进行重新连接。第三种类似第二种方式。

Enter path to handshake file



\part{Tool}


\chapter{Docker}

\section{Docker基础}

\subsection{为什么要用Docker}

好不容易配置部署了redmine，结果后面换了一台机器，又重新部署一遍？其实绝大部都是配置的工作，繁琐又重复，其实我们没有办法也没有必要记住每一步操作步骤，先安装什么，安装好了再修改哪个文件的配置，重复的劳动没有任何意义。开发过程中会发现，许多工作都可以总结出来套路的，做过一遍之后，以后的内容都是高度重复。比如安装个软件，部署个应用，特别时多机部署，要保证一致性非常麻烦。还有经常会有的问题：在我的电脑上好好的呀？以上这些问题，就是Docker要解决的问题，可大量减少花费在维护、配置不同系统的时间。常用命令:

\begin{lstlisting}[language=Bash]
# 查看当前运行的容器
docker ps
# 登陆Docker容器
docker exec -it f5605d02f9f5 bash
# 登陆Docker后，登陆MariaDB
mysql -h127.0.0.1 -uroot -p123456
\end{lstlisting}


\subsection{Docker安装}

\subsection{安装数据层}

查看MariaDB:

\begin{lstlisting}[language=Bash]
docker search mariadb
\end{lstlisting}

安装MariaDB:

\begin{lstlisting}[language=Bash]
docker pull mariadb
\end{lstlisting}

安装时，从国外镜像下载的速非常非常慢，首先将下载源调整为国内的下载源。对于使用systemd的系统，在/etc/docker/daemon.json中写入如下内容(不存在此文件新建即可)：

\begin{lstlisting}
{
	"registry-mirrors": [
		"https://registry.docker-cn.com"
	]
}
\end{lstlisting}

运行容器：

\begin{lstlisting}[language=Bash]
docker run --name MariaDB \
	-p 3306:3306 \
	-v /data/db/mariadb:/var/lib/mysql \
	-e MYSQL_ROOT_PASSWORD=123456 \
	-d mariadb
# 运行Jenkins
docker run -p 8080:8080 \ 
	-p 50000:50000 \ 
	-v /usr/local/work/jenkins:/var/jenkins_home \ 
	--name j01 -idt jenkins
\end{lstlisting}

p表示要自定义映射的端口(port)，可以用-p hostPort:containerPort。为了方便迁移数据库中的数据，可以通过挂载数据卷(volum	e)来实现。这样，数据库中的数据将保存在我们挂载的本地文件/data/db/mariadb的上。我们可以迁移或者备份这个文件夹，来实现数据库迁移。一般一个自动生成的空数据库文件，大概有100多兆 ，而且这个文件夹中包含很多子文件，因此如果通过SSH或者FTP传输都需要比较长的时间，可以通过压缩打包来减少文件夹的容量。启动MariaDB镜像：

\begin{lstlisting}[language=Bash]
# 启动MariaDB
docker run --name mariadb -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -d mariadb
\end{lstlisting}



\subsection{Docker导入导出}

查看名称：

\begin{lstlisting}[language=Bash]
docker ps
\end{lstlisting}

导出：

\begin{lstlisting}[language=Bash]
docker import -o MariaDB.tar MariaDB
\end{lstlisting}

导入：

\begin{lstlisting}[language=Bash]
docker export MariaDB.tar MariaDB
\end{lstlisting}

\begin{thebibliography}{0}
\bibitem{蒋金楠MVC框架揭秘}
蒋金楠.《ASP.NET MVC 5框架揭秘》.电子工业出版社,2014.
\end{thebibliography}

%\bibitem{BSD}
%B. Birch, H. P. F. Swinnerton-Dyer, \emph{Notes on ellptic curves (II)}. J Reine Angrew Math, 1965, 218:79-108.
\end{document}
